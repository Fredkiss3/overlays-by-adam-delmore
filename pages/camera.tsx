import type { NextPage } from "next"
import Head from "next/head"
import { Notification } from "../components/notification"
import { useCallback, useEffect, useState } from "react"
import React from "react"
import * as THREE from "three"
import {
  Face,
  FaceLandmarksDetector,
} from "@tensorflow-models/face-landmarks-detection"
import { Canvas } from "@react-three/fiber"
import {
  fps,
  HeartGlasses,
  StPatrickHat,
  videoHeight,
  videoWidth,
} from "../components/models"
import { useRouter } from "next/router"
import { TwitchEvent } from "../lib/twitch"

const websocketUrl = "ws://localhost:3333"
type Device = "primary" | "secondary"

let detector: FaceLandmarksDetector

const CameraPage: NextPage = () => {
  const router = useRouter()
  const debug = router.query.debug === "true"

  const [loaded, setLoaded] = useState(false)
  const videoRef = React.useRef<HTMLVideoElement | null>(null)
  const cameraRef = React.useRef<THREE.PerspectiveCamera>(null)
  const [faces, setFaces] = useState<Face[]>()

  const trackFace = useCallback(async () => {
    if (!loaded || !videoRef.current || !detector) return

    const faces = await detector.estimateFaces(videoRef.current, {
      flipHorizontal: true,
      staticImageMode: false,
    })
    setFaces(faces)
  }, [loaded])

  useEffect(() => {
    async function init() {
      const video = videoRef.current
      if (!video) return

      const cameras = await navigator.mediaDevices.enumerateDevices()
      const camlinkCameras = cameras.filter(
        (camera) =>
          camera.kind === "videoinput" && camera.label.startsWith("Cam Link 4K")
      )

      const deviceName = router.query.device as Device
      const deviceIndex = deviceName === "primary" ? 0 : 1
      const desiredCamera = camlinkCameras[deviceIndex]

      const userMedia = await navigator.mediaDevices.getUserMedia({
        video: {
          deviceId: desiredCamera
            ? { exact: desiredCamera.deviceId }
            : undefined,
        },
      })

      video.srcObject = userMedia
      video.play()

      detector = await (await import("../lib/detector")).createDetector()
    }

    init()
  }, [router.query.device])

  useEffect(() => {
    const camera = cameraRef.current
    if (!camera) return

    camera.lookAt(videoWidth / 2, -videoHeight / 2, 0)
  }, [cameraRef])

  useEffect(() => {
    const timer = setInterval(trackFace, 1000 / fps)
    return () => clearInterval(timer)
  }, [trackFace])

  return (
    <div className="relative h-[1080px] w-[1920px]">
      <Head>
        <title>Adam&apos;s Twitch Overlay</title>
        <meta name="description" content="Generated by create next app" />
        <link rel="icon" href="/favicon.ico" />
      </Head>
      <video
        autoPlay
        muted
        width={videoWidth}
        height={videoHeight}
        ref={videoRef}
        className={`absolute inset-0 ${!debug && "invisible"}`}
        onLoadedData={() => setLoaded(true)}
      />
      <Canvas className="absolute inset-0 w-full h-full top-0 right-0">
        <ambientLight />
        <pointLight position={[10, 10, 10]} />
        <perspectiveCamera
          ref={cameraRef}
          fov={45}
          position={[
            videoWidth / 2,
            -videoHeight / 2,
            -(videoHeight / 2) / Math.tan(45 / 2),
          ]}
        />
        {faces?.map((face, i) => {
          const camera = cameraRef.current

          return (
            <>
              {debug && (
                <>
                  <HeartGlasses key={i} face={face} camera={camera} />
                  <StPatrickHat
                    key={`${i}-st-patrick-hat`}
                    face={face}
                    camera={camera}
                  />
                </>
              )}
            </>
          )
        })}
      </Canvas>
    </div>
  )
}

export default CameraPage
